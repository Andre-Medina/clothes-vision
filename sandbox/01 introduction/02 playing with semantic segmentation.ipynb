{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /mnt/e/clothes/clothes-vision/sandbox/01 introduction/fashionpedia/fashionpedia.py or any data file in the same directory. Couldn't find 'fashionpedia' on the Hugging Face Hub either: FileNotFoundError: Dataset 'fashionpedia' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_625/3078829240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load Fashionpedia dataset from Hugging Face Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fashionpedia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../data/fashionpedia/fashionpedia-api\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Define some constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1786\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1515\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m                     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1228\u001b[0m                         \u001b[0;34mf\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /mnt/e/clothes/clothes-vision/sandbox/01 introduction/fashionpedia/fashionpedia.py or any data file in the same directory. Couldn't find 'fashionpedia' on the Hugging Face Hub either: FileNotFoundError: Dataset 'fashionpedia' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load Fashionpedia dataset from Hugging Face Datasets\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"fashionpedia\", data_dir = \"../../data/fashionpedia/fashionpedia-api\")\n",
    "\n",
    "# Define some constants\n",
    "NUM_CATEGORIES = 27 # Number of main apparel categories\n",
    "NUM_PARTS = 19 # Number of apparel parts\n",
    "NUM_ATTRIBUTES = 294 # Number of fine-grained attributes\n",
    "IMAGE_SIZE = (1024, 1024) # Size of input images\n",
    "BATCH_SIZE = 4 # Batch size for training and evaluation\n",
    "EPOCHS = 50 # Number of epochs for training\n",
    "LEARNING_RATE = 0.001 # Learning rate for optimizer\n",
    "CHECKPOINT_DIR = \"checkpoints\" # Directory to save model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to preprocess the images and masks\n",
    "def preprocess_data(example):\n",
    "  # Decode image from bytes and resize to IMAGE_SIZE\n",
    "  image = tf.image.decode_jpeg(example[\"image\"])\n",
    "  image = tf.image.resize(image, IMAGE_SIZE)\n",
    "  image = tf.cast(image, tf.float32) / 255.0 # Normalize to [0, 1] range\n",
    "\n",
    "  # Decode mask from bytes and resize to IMAGE_SIZE\n",
    "  mask = tf.io.decode_raw(example[\"segmentation_mask\"], tf.uint8)\n",
    "  mask = tf.reshape(mask, example[\"segmentation_mask_shape\"])\n",
    "  mask = tf.image.resize(mask, IMAGE_SIZE, method=\"nearest\")\n",
    "  mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "  # Get attributes for each mask\n",
    "  attributes = example[\"attributes\"]\n",
    "\n",
    "  return image, mask, attributes\n",
    "\n",
    "# Create input pipelines for training and validation splits\n",
    "train_dataset = dataset[\"train\"].map(preprocess_data)\n",
    "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = dataset[\"validation\"].map(preprocess_data)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Attribute-Mask R-CNN model\n",
    "def get_attribute_mask_rcnn(num_categories, num_parts, num_attributes):\n",
    "\n",
    "  # Create a ResNet-50 backbone with FPN feature extractor\n",
    "  backbone = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\")\n",
    "  backbone.trainable = False # Freeze backbone weights\n",
    "  inputs = tf.keras.Input(shape=[None, None, 3])\n",
    "  x = backbone(inputs)\n",
    "  x = tf.keras.layers.Conv2D(256, (1,1))(x)\n",
    "  \n",
    "  # Create a feature pyramid network (FPN) layer\n",
    "  fpn = pix2pix.FeaturePyramidNetwork([64,128,256], num_filters=256)\n",
    "\n",
    "  # Create a region proposal network (RPN) layer\n",
    "  rpn = pix2pix.RPNLayer(num_anchors=9)\n",
    "\n",
    "  # Create a ROI align layer\n",
    "  roi_aligner = pix2pix.PyramidROIAlign([14,14])\n",
    "\n",
    "  # Create a detection head layer\n",
    "  detection_head = pix2pix.DetectionHead(num_classes=num_categories+num_parts+1)\n",
    "\n",
    "  # Create a mask head layer\n",
    "  mask_head = pix2pix.MaskHead(num_classes=num_categories+num_parts+1)\n",
    "\n",
    "  # Create an attribute head layer\n",
    "  attribute_head = pix2pix.AttributeHead(num_attributes=num_attributes)\n",
    "\n",
    "  # Connect the layers to form the model\n",
    "  features = fpn(x)\n",
    "  rpn_class, rpn_bbox, anchors = rpn(features)\n",
    "  proposals = pix2pix.apply_box_deltas(anchors, rpn_bbox)\n",
    "  proposals = tf.clip_by_value(proposals, 0, 1)\n",
    "  rois = pix2pix.batch_slice([proposals, rpn_class], lambda x,y: pix2pix.proposal_filter(x,y), 4)\n",
    "  roi_features = roi_aligner([rois]+features)\n",
    "  detections = detection_head(roi_features)\n",
    "  masks = mask_head(roi_features)\n",
    "  attributes = attribute_head(roi_features)\n",
    "\n",
    "  # Define the model inputs and outputs\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=[detections, masks, attributes])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Create the model\n",
    "model = get_attribute_mask_rcnn(NUM_CATEGORIES, NUM_PARTS, NUM_ATTRIBUTES)\n",
    "\n",
    "# Define the loss functions\n",
    "def rpn_class_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are (batch_size, num_anchors) tensors\n",
    "  # Compute binary cross-entropy loss for foreground/background classification\n",
    "  loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "  # Ignore anchors that are not positive or negative (y_true=-1)\n",
    "  anchor_state = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "  loss = tf.reduce_sum(loss * anchor_state) / (tf.reduce_sum(anchor_state) + 1e-3)\n",
    "  return loss\n",
    "\n",
    "def rpn_bbox_loss(target_bbox, rpn_class, pred_bbox):\n",
    "  # target_bbox and pred_bbox are (batch_size, num_anchors, (dy, dx, log(dh), log(dw))) tensors\n",
    "  # rpn_class is (batch_size, num_anchors) tensor\n",
    "  # Compute smooth L1 loss for bounding box regression\n",
    "  diff = tf.abs(target_bbox - pred_bbox)\n",
    "  less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
    "  loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "  # Only consider positive anchors (rpn_class=1)\n",
    "  anchor_state = tf.cast(tf.equal(rpn_class, 1), tf.float32)\n",
    "  loss = tf.reduce_sum(loss * anchor_state) / (tf.reduce_sum(anchor_state) + 1e-3)\n",
    "  return loss\n",
    "\n",
    "def detection_loss(target_class_ids, pred_class_logits, target_deltas, pred_deltas):\n",
    "    # target_class_ids and pred_class_logits are (batch_size, num_rois) tensors\n",
    "    # target_deltas and pred_deltas are (batch_size, num_rois, (dy, dx, log(dh), log(dw))) tensors\n",
    "    # Compute losses for the detection head\n",
    "    class_loss = tf.keras.losses.sparse_categorical_crossentropy(target_class_ids, pred_class_logits)\n",
    "    class_loss = tf.reduce_mean(class_loss)\n",
    "    \n",
    "    target_class_ids = tf.cast(target_class_ids > 0 ,tf.float32)\n",
    "    \n",
    "    diff = tf.abs(target_deltas - pred_deltas)\n",
    "    \n",
    "\n",
    "    less_than_one = tf.cast(tf.less(diff,1.0),tf.float32)\n",
    "\n",
    "    bbox_loss=(less_than_one*0.5*diff**2)+(1-less_than_one)*(diff-0.5)\n",
    "\n",
    "    bbox_loss=tf.reduce_sum(bbox_loss*target_class_ids)/tf.maximum(tf.reduce_sum(target_class_ids),1)\n",
    "\n",
    "    return class_loss,bbox_loss\n",
    "\n",
    "def mask_loss(target_masks,pred_masks,target_class_ids):\n",
    "\n",
    "    #target_masks and pred_masks are(batch_size,num_rois,height,width) tensors\n",
    "    #target_class_ids is(batch_size,num_rois) tensor\n",
    "    #Compute binary cross-entropy loss for mask prediction\n",
    "\n",
    "    loss=tf.keras.losses.binary_crossentropy(target_masks,pred_masks)\n",
    "\n",
    "    #Only consider positive ROIs(target_class_ids>0)\n",
    "\n",
    "    target_class_ids=tf.reshape(target_class_ids,[tf.shape(loss)[0],tf.shape(loss)[1],1])\n",
    "\n",
    "    loss=tf.reduce_sum(loss*target_class_ids)/tf.maximum(tf.reduce_sum(target_class_ids),1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def attribute_loss(target_attributes,pred_attributes,target_mask_ids):\n",
    "\n",
    "    #target_attributes and pred_attributes are(batch_size,num_rois,num_attributes) tensors\n",
    "\n",
    "    #target_mask_ids is(batch_size,num_rois) tensor\n",
    "\n",
    "    #Compute binary cross-entropy loss for attribute prediction\n",
    "\n",
    "    loss=tf.keras.losses.binary_crossentropy(target_attributes,pred_attributes)\n",
    "\n",
    "    #Only consider positive masks(target_mask_ids>0)\n",
    "\n",
    "    target_mask_ids=tf.reshape(target_mask_ids,[tf.shape(loss)[0],tf.shape(loss)[1],1])\n",
    "\n",
    "    loss=tf.reduce_sum(loss*target_mask_ids)/tf.maximum(tf.reduce_sum(target_mask_ids),1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "#Define the optimizer and learning rate schedule\n",
    "# Define the optimizer and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Define a function to compute the total loss\n",
    "def compute_loss(model, inputs):\n",
    "  # Unpack the inputs\n",
    "  images, masks, attributes = inputs\n",
    "\n",
    "  # Get the model outputs\n",
    "  detections, pred_masks, pred_attributes = model(images)\n",
    "\n",
    "  # Get the RPN outputs\n",
    "  rpn_class_logits, rpn_class, rpn_bbox = model.layers[2].output\n",
    "  anchors = model.layers[2].anchors\n",
    "\n",
    "  # Get the detection outputs\n",
    "  pred_class_logits, pred_class_ids, pred_deltas = detections\n",
    "\n",
    "  # Generate targets for RPN\n",
    "  rpn_match, rpn_bbox = pix2pix.build_rpn_targets(anchors, masks)\n",
    "  \n",
    "  # Generate targets for detection head\n",
    "  target_class_ids, target_deltas, target_masks = pix2pix.build_detection_targets(detections, masks)\n",
    "\n",
    "  # Generate targets for attribute head\n",
    "  target_attributes, target_mask_ids = pix2pix.build_attribute_targets(attributes, masks)\n",
    "\n",
    "  # Compute losses for RPN\n",
    "  rpn_class_loss_value = rpn_class_loss(rpn_match, rpn_class)\n",
    "  rpn_bbox_loss_value = rpn_bbox_loss(rpn_bbox, rpn_match, rpn_class_logits)\n",
    "\n",
    "  # Compute losses for detection head\n",
    "  detection_class_loss_value, detection_bbox_loss_value = detection_loss(target_class_ids, pred_class_logits, target_deltas, pred_deltas)\n",
    "\n",
    "  # Compute loss for mask head\n",
    "  mask_loss_value = mask_loss(target_masks, pred_masks, target_class_ids)\n",
    "\n",
    "  # Compute loss for attribute head\n",
    "  attribute_loss_value = attribute_loss(target_attributes, pred_attributes, target_mask_ids)\n",
    "\n",
    "  # Compute total loss\n",
    "  total_loss = rpn_class_loss_value + rpn_bbox_loss_value + detection_class_loss_value + detection_bbox_loss_value + mask_loss_value + attribute_loss_value\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "# Define a function to perform a training step\n",
    "@tf.function\n",
    "def train_step(model, inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Compute the loss value for this batch.\n",
    "    loss_value = compute_loss(model, inputs)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the model's trainable variables.\n",
    "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    # Update the weights of the model.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "# Define a function to perform an evaluation step\n",
    "@tf.function\n",
    "def val_step(model, inputs):\n",
    "  # Compute the loss value for this batch.\n",
    "  loss_value = compute_loss(model, inputs)\n",
    "  \n",
    "  return loss_value\n",
    "\n",
    "# Define a checkpoint manager to save the best model\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_DIR , max_to_keep=3)\n",
    "\n",
    "# Define some variables to track the training progress\n",
    "best_val_loss = np.inf # The best validation loss so far\n",
    "patience = 0 # The number of epochs without improvement in validation loss\n",
    "stop_training = False # A flag to stop the training loop\n",
    "\n",
    "# Define metrics to track the train and val losses\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "# Start the training loop\n",
    "for epoch in range(EPOCHS):\n",
    "  \n",
    "  print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "  \n",
    "  # Reset the metrics at the start of each epoch\n",
    "  train_loss.reset_states()\n",
    "  val_loss.reset_states()\n",
    "  \n",
    "  # Loop over the batches of the training dataset.\n",
    "  for inputs in train_dataset:\n",
    "    \n",
    "    # Perform a training step and update the train loss metric.\n",
    "    train_loss.update_state(train_step(model, inputs))\n",
    "    \n",
    "    # Display the current train loss\n",
    "    print(f\"\\rTrain Loss: {train_loss.result():.4f}\", end=\"\")\n",
    "    \n",
    "  print()\n",
    "  \n",
    "  # Loop over the batches of the validation dataset.\n",
    "  for inputs in val_dataset:\n",
    "    \n",
    "    # Perform an evaluation step and update the val loss metric.\n",
    "    val_loss.update_state(val_step(model, inputs))\n",
    "    \n",
    "    # Display the current val loss\n",
    "    print(f\"\\rVal Loss: {val_loss.result():.4f}\", end=\"\")\n",
    "    \n",
    "  print()\n",
    "  \n",
    "  # Compare the current val loss with the best val loss and update accordingly\n",
    "  if val_loss.result() < best_val_loss:\n",
    "    \n",
    "    print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss.result():.4f}. Saving checkpoint.\")\n",
    "    best_val_loss = val_loss.result()\n",
    "    patience = 0\n",
    "    manager.save()\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    print(f\"Validation loss did not improve from {best_val_loss:.4f}.\")\n",
    "    patience += 1\n",
    "    if patience > 5:\n",
    "      print(\"Early stopping.\")\n",
    "      stop_training = True\n",
    "      break\n",
    "    \n",
    "  if stop_training:\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
